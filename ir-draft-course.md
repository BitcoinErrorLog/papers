### ChapterÂ 1Â â€“Â InstantÂ InvertedÂ Index

**Purpose**Â Â Build a bareâ€‘bones search service that ingests Pubky posts, produces an [inverted index](https://en.wikipedia.org/wiki/Inverted_index), and returns topâ€‘k hits inÂ <50â€¯ms.

**Why it matters**Â Â Every later stageâ€”[BM25](https://en.wikipedia.org/wiki/Okapi_BM25) tuning, [embeddings](https://en.wikipedia.org/wiki/Word_embedding), and graph rerankingÂ ([learningÂ toÂ rank](https://en.wikipedia.org/wiki/Learning_to_rank))â€”rests on a solid lexical index. Master this and you control the basic plumbing of PubkyÂ Search.

---

\####Â 1.Â Concepts you must ownâ€¯â€”â€¯inÂ 30Â min

* **[Tokenisation](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)**Â â€“ splitting post bodies, titles and `tags[]` into lowerâ€‘cased terms.
* **[Stopâ€‘wordÂ removal](https://en.wikipedia.org/wiki/Stop_words)**Â â€“ filter highâ€‘frequency noise (e.g. â€œtheâ€, â€œandâ€).
* **[PostingÂ list](https://en.wikipedia.org/wiki/Inverted_index#Postings_lists)**Â â€“ map `termÂ â†’Â [(docID,Â tf),â€¦]`.
* **Topâ€‘k retrieval**Â â€“ intersect posting lists, rank by termâ€‘frequency, return firstÂ k.

---

\####Â 2.Â Pubky specifics

* **Post source**Â â€“ stream JSON posts from your homeserverâ€™s `/posts?since=` endpoint.
* **DocID scheme**Â â€“ use `<pubkey>:<postHash>`.
* **Tags are terms**Â â€“ treat each signed tag as a highâ€‘weight term; you will boost them later.

---

\####Â 3.Â LabÂ #1Â â€“Â Ship a working index

1. **Fetch corpus**Â Â `npx pubky-cli fetch --limit 10000 > corpus.json`
2. **Build index**Â Â Node/TypeScript:

   ```ts
   for (const post of corpus) {
     for (const term of tokenize(post.body + ' ' + post.tags.join(' '))) {
       addToInverted(term, post.id);
     }
   }
   ```
3. **Serve search**Â Â Fastify route `GET /search?q=` parses query â†’ intersects lists â†’ responds JSON.
4. **Benchmark**Â Â Aim for P50 latency <50â€¯ms, memoryÂ <300â€¯MBÂ @Â 10Â k docs.

---

\####Â 4.Â Deliverable checklist

* `index.ts`, `server.ts`, `README.md` with run instructions.
* Demo: `curl "localhost:3000/search?q=bitcoin+tag:lightning"` returns IDs and snippet.

---

\####Â 5.Â Quick lookâ€‘ups

| Term               | Oneâ€‘line definition              |
| ------------------ | -------------------------------- |
| **InvertedÂ index** | Hashâ€‘map from termÂ â†’Â docIDs      |
| **PostingÂ list**   | The array `[docID,Â tf]` per term |
| **Stopâ€‘word**      | Superâ€‘common term you drop       |

### ChapterÂ 2Â â€“Â SmartÂ Weighting

**Purpose**Â Â Replace raw termâ€‘frequency scoring with BM25 tuned for Pubkyâ€™s short, tagâ€‘rich posts, boosting retrieval precision without extra latency.

**Prerequisite**Â Â Running invertedâ€‘index service from ChapterÂ 1.

---

#### 1. Concepts you need today

* **[TF](https://en.wikipedia.org/wiki/Term_frequency), [DF](https://en.wikipedia.org/wiki/Inverted_index#Document_frequency), [IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency)** â€“ recap the three signals behind BM25.
* **BM25 hyperâ€‘parameters** â€“ `k1` controls termâ€‘frequency saturation; `b` controls length normalisation.
* **Document length distribution** â€“ Pubky posts averageÂ 140â€¯chars; set `bâ‰ˆ0.25` to avoid overâ€‘penalising short notes.

#### 2. Pubkyâ€‘specific insights

* Tags appear as firstâ€‘class stringsâ€”index them **twice** (fieldâ€‘boost) so tag matches outrank body matches.
* Use Pubky SDK stream to collect average post length each night; autoâ€‘adjust `avgdl`.

#### 3. Lab â€“ Tune and benchmark

1. Integrate BM25 into `rank.ts`.
2. Gridâ€‘search `k1 âˆˆ {0.9,Â 1.1,Â 1.3}` and `b âˆˆ {0.15,Â 0.25,Â 0.35}` on a 500â€‘query test set curated from Pubky App beta logs.
3. Compute **[P@10](https://en.wikipedia.org/wiki/Information_retrieval#Evaluation_metrics)** and **[nDCG](https://en.wikipedia.org/wiki/DCG)**; choose best.
4. Commit param file `bm25.json` for later stages.

#### 4. Success criteria

* â‰¥Â +8â€¯% P\@10 vs. ChapterÂ 1 baseline.
* 95â€‘th percentile latency still <60â€¯ms.

---

**Takeâ€‘home**Â Â Congratulationsâ€”you now have an industrialâ€‘strength lexical ranker ready for semantic addâ€‘ons.

### ChapterÂ 3Â â€“Â SemanticÂ EmbeddingsÂ atÂ ProductionÂ Speed

**Purpose**Â Â Catch lexical misses by embedding content & queries into a shared vector space and retrieving via an [approximateâ€‘nearestâ€‘neighbour (ANN)](https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximate_nearest_neighbor_%28ANN%29_search) index.

**Builds on**Â Â BM25â€‘backed search API from ChapterÂ 2.

---

#### 1. Key takeâ€‘aways

* **[Sentenceâ€‘BERT](https://www.sbert.net/) / [MiniLM](https://huggingface.co/transformers/model_doc/minilm.html)** â€“ readyâ€‘toâ€‘use models that fit in <100â€¯MB and return 384â€‘dim vectors.
* **ANN search ([HNSW](https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world_graph))** â€“ smallâ€‘world graph structure delivering subâ€‘100â€¯ms recalls at millionâ€‘scale.
* **Hybrid search pattern** â€“ lexical first; if max BM25 < Ï„, fall back to semantic results.

---

#### 2. Pubky deployment specifics

* Store the 384â€‘vec for each post alongside its invertedâ€‘index entry.
* Tags are concatenated to body text before embedding to preserve user intent.
* Persist vectors in **[Faiss](https://github.com/facebookresearch/faiss)** on disk; load at startâ€‘up.

---

#### 3. Handsâ€‘on lab steps

1. **Fineâ€‘tune**Â MiniLM on a 50â€¯k postâ€“tag parallel corpus (optional but yields +12â€¯% recall).
2. Embed all posts: `embedder.encode([title + " " + tags + " " + body])` â†’ `vectors.bin`.
3. Build HNSW: `faiss.IndexHNSWFlat(d=384, M=32)`; add vectors.
4. Modify `/search` endpoint:

   * Compute BM25; if top score â‰¥Â Ï„ (defaultÂ 2.0) serve lexical ranking.
   * Else embed query and fetch 100 nearest neighbours; merge results with BM25 list, dedup.
5. Measure latency and recall vs. ChapterÂ 2 baseline.

---

#### 4. Success criteria

* <150â€¯ms p95 latency for hybrid path
* â‰¥20â€¯% relative recall gain on manual relevance judgements.


### ChapterÂ 4Â â€“Â Graphâ€‘AwareÂ Reranking

**Purpose**Â Â Leverage Pubkyâ€™s Semantic Social Graph to rerank search results by trust and social proximity, turning a good lexical/semantic match into a great personalised hit list.

**Builds on**Â Â Hybrid lexicalÂ + semantic engine from ChapterÂ 3.

---

#### 1. Core ideas

* **Tripartite graph** â€“ usersâ€¯â†”â€¯tagsâ€¯â†”â€¯postsÂ îˆ€citeîˆ‚turn2file3îˆ.
* **Trust propagation** â€“ compute a [PageRank](https://en.wikipedia.org/wiki/PageRank)-style centrality score, seeded by the searcherâ€™s Webâ€‘ofâ€‘Trust (WoT) peers.
* **Edge weighting** â€“ strength derives from signature trust + tag relevance.

#### 2. Minimal theory (15â€¯min)

1. Random walks on graphs converge to a stationary distribution proportional to node centrality.
2. PageRank with a personalised restart vector becomes **TrustRank**â€”higher weight for edges originating from trusted users, damped byÂ Î±â€¯â‰ˆâ€¯0.85.
3. Reranking rule: `score_finalÂ =Â score_lexsemÂ Ã—Â (1Â +Â Î»Â Ã—Â trust_rank)` where Î» is tuned on clickâ€‘through.

#### 3. Handsâ€‘on lab

1. **Export graph**Â â€“ From your Chapterâ€¯3 index, emit `(user_id, tag, post_id, weight)` triples in GraphML.
2. **Compute TrustRank**Â â€“ Run powerâ€‘iteration or GraphX with a personalised seed vector.
3. **Plug into reranker**Â â€“ Multiply BM25/embedding score by trust factor; compare P\@10 lift on heldâ€‘out queries.

#### 4. What to ship

* `trust_rank.py`Â â€“ script that ingests GraphML, outputs numpy array of trust scores.
* Modified search microservice that injects trust scores during ranking stage.
* A quick dashboard showing before/after metrics (precision, clickâ€‘through).

---

**You now have personalised, trustâ€‘boosted search that outperforms lexical+semantic alone.**

Next up: harness Pubkyâ€™s signed tags for even deeper relevance signals.

### ChapterÂ 5Â â€“Â SignedÂ SocialÂ TagsÂ asÂ RankingÂ Signals

**Purpose**Â Â Exploit cryptographically signed tags as firstâ€‘class IR features for discovery, recommendation, moderation, and personalised ranking.

**Builds on**Â Â Graph trust scores and reranker from ChapterÂ 4.

---

#### 1. Mustâ€‘know primitives (new since previous chapters)

* **Tag namespace & scope**Â â€“ tags are relative to the signer and contextÂ îˆ€citeîˆ‚turn2file4îˆ.
* **Signature payload**Â â€“ publicâ€‘key, tag, target, timestamp (enables cryptographic provenance).
* **[Coâ€‘occurrenceÂ matrix](https://en.wikipedia.org/wiki/Co-occurrence)**Â â€“ counts how often two tags appear together across posts; foundation for measuring association strength.
* **[Folksonomy](https://en.wikipedia.org/wiki/Folksonomy)**Â â€“ a userâ€‘generated tagging system rather than a controlled taxonomy.
* **[ControlledÂ vocabulary](https://en.wikipedia.org/wiki/Controlled_vocabulary)**Â â€“ a curated set of allowed terms; contrasts with freeâ€‘form folksonomy and helps disambiguate tags.
* **[Coldâ€‘startÂ problem](https://en.wikipedia.org/wiki/Cold_start_problem)**Â â€“ difficulty recommending or ranking items that have few interactions or tags.
* **[PointwiseÂ MutualÂ InformationÂ (PMI)](https://en.wikipedia.org/wiki/Pointwise_mutual_information)**Â â€“ statistic for measuring association between two discrete events; used to weight tag pairs.

---

#### 2. What to implement

1. **Signedâ€‘Tag Ingest** â€“ extend your ingest worker to parse `sig`, `tag`, `target`, `pubkey`, `ts` fields and persist them in a `tags` table keyed by `(tag, target)`.
2. **Tag Coâ€‘occurrence Job** â€“ daily Spark / DuckDB job that outputs a tagÃ—tag PMI matrix for later query expansion (used in ChapterÂ 7).
3. **Tagâ€‘Aware Scoring** â€“ during ranking, add a bonus weight when a post contains a tag issued by a user inside the searcherâ€™s Webâ€‘ofâ€‘Trust.
4. **Tag Recommendation Endpoint** â€“ `GET /tags/suggest?u=<pubkey>&k=10` returns k candidate tags based on coâ€‘occurrence + trust weights.

---

#### 3. Lab

> **Goal:**Â Ship a tag recommender that suggests five new tags to any Pubky user based on their historical tagging behaviour and coâ€‘signers.

*StepÂ A*Â  Run the coâ€‘occurrence job on a 1â€‘week post dump.
*StepÂ B*Â  Compute PMI scores and keep the top 1â€¯000 tag pairs.
*StepÂ C*Â  For a test user, merge their personal tag history with PMIâ€‘suggested tags; boost suggestions from signers within two hops of their trust graph.
*StepÂ D*Â  Evaluate coverage and precision offline; aim for >40â€¯% acceptance rate in a simulated "accept/reject" user study.

---

#### 4. Quick checklist

* &#x20;Parse and verify tag signatures (Ed25519).
* Materialise tag co-occurrence & PMI.
* Integrate trust-weighted bonus into your BM25 score.
* Expose /tags/suggest API.\\

  Complete these steps and youâ€™ll feed richer, high-precision tag signals into the ranking pipeline you already have.

### ChapterÂ 6Â â€“Â DistributedÂ IndexingÂ withÂ Nexus

**Purpose**Â Â Shard and replicate your search index across Nexus nodes, announced and discovered through PKARR on the Mainline DHT, so Pubky Search keeps working when a single boxâ€”or an entire dataâ€‘centreâ€”drops offline.

**Builds on**Â Â Local index pipeline from ChaptersÂ 1â€‘5.

---

#### 1. Concepts in one sitting

* **[DistributedÂ hashÂ tableÂ (DHT)](https://en.wikipedia.org/wiki/Distributed_hash_table)**Â â€“ keyâ†’value overlay network enabling lookups without a central registry.
* **MainlineÂ DHTÂ &Â PKARR**Â â€“ Pubkyâ€™s flavour of Mainline; PKARR stores a "resource record" that maps a logical shardâ€‘ID to `multiaddr`s where it livesÂ îˆ€citeîˆ‚turn2file7îˆ.
* **ShardÂ key**Â â€“ result of hashing `postID` (or a tag, depending on index) and applying **[consistentÂ hashing](https://en.wikipedia.org/wiki/Consistent_hashing)** to divvy the space evenly.
* **ReplicaÂ factor**Â â€“ how many nodes store the same shard; drives durability and read availability. See **[replicationÂ factor](https://en.wikipedia.org/wiki/Replication_%28computing%29)**.
* **[CAPÂ theorem](https://en.wikipedia.org/wiki/CAP_theorem)**Â â€“ why perfect consistency, availability, and partition tolerance canâ€™t all be guaranteed at once.
* **[GossipÂ protocol](https://en.wikipedia.org/wiki/Gossip_protocol)**Â â€“ how Nexus nodes share routing tables and freshness metadata.

---

#### 2. Architecture snapshot

```
 client âœ gateway âœ queryâ€‘router â”
                         shardâ€‘A ğŸ—‚ï¸  â”€â”€â”
                         shardâ€‘B ğŸ—‚ï¸  â”€â”€â”¼â”€â–¶ BM25 + embedding + graph rerank
                         shardâ€‘C ğŸ—‚ï¸  â”€â”€â”˜
                 (each shard served by â‰¥ replicaâ€‘factor nodes)

 shards announce  âœ MainlineÂ DHT/PKARR âœ discovery cache in gateway
```

*The gateway keeps a 30Â s cache of shardâ€‘IDÂ â†’ node addresses to avoid DHT roundâ€‘trips per query.*

---

#### 3. Lab â€“Â Spin up your first Nexus swarm

1. **Bootstrap two nodes** on different ports; run the `nexus-indexer` container with envÂ `REPLICA_FACTOR=2`.
2. **Shard the index** produced in ChapterÂ 5 with consistent hashing (e.g. 64 shards).
3. **Announce** each shardâ€‘ID via PKARR (`nexus announce --shard 17 --multiaddr /ip4/â€¦`).
4. **Query router**: modify your Chapterâ€‘3 search API so it can resolve a shardâ€‘ID âœ node list via PKARR, pick the fastest replica (basic RTT probe) and failover after 150Â ms.
5. **Chaos test**: kill one node; confirm queries still succeed (albeit a little slower) via the surviving replica.

Expected outcome: latency budget rises only \~30â€¯ms due to network hop + cache miss; uptime â‰¥Â 99.9â€¯% under oneâ€‘node failure.

---

#### 4. Quick checklist

* Pick a shard count (power of two) and seed the consistent-hashing ring.
* Set REPLICA\_FACTOR â‰¥ 2; monitor disk-usage growth.
* Enable shard-freshness gossip every 60 s.
* Add retry/back-off logic in the router before surfacing 5XX to the client.
* Record per-shard QPS, tail latency, and error rate in Prometheus.

  Complete these bullets and your index can now survive node churn and geographic splits without users noticing a thing.

### ChapterÂ 7Â â€“Â ContextualÂ QueryÂ Expansion

**Purpose**Â Â Recover otherwiseâ€‘missed results when lexicalâ€¯+â€¯semantic matches fail by automatically expanding the userâ€™s query with contextually relevant tags, synonyms, and entities drawn from their Webâ€‘ofâ€‘Trust.

**Builds on**Â Â Graph analytics (ChapterÂ 4), signedâ€‘tag signals (ChapterÂ 5), and the distributed Nexus index (ChapterÂ 6).

---

#### 1. Fastâ€‘track theory (new concepts only)

* **Local synonym table**Â â€“ offline map from tag â†” term coâ€‘occurrences in the Pubky corpus (PMIÂ â‰¥Â 1.5).
* **[Pseudoâ€‘relevanceÂ feedback](https://en.wikipedia.org/wiki/Pseudo-relevance_feedback)**Â â€“ assume topâ€‘k initial results are relevant; mine expansion terms from them.
* **[RocchioÂ algorithm](https://en.wikipedia.org/wiki/Rocchio_algorithm)**Â â€“ classic vectorâ€‘space method to reâ€‘weight query vectors after feedback.
* **[Queryâ€‘likelihoodÂ model](https://en.wikipedia.org/wiki/Language_model_for_information_retrieval)**Â â€“ probabilistic approach that scores docs by the likelihood they would generate the query; useful baseline for comparing expansion gains.

---

#### 2. Practical algorithm (5â€‘step loop)

1. **Initial pass** â†’ run existing lexicalÂ + semantic search; collect topâ€¯k=20.
2. **Candidate terms** â†’ extract highâ€‘PMI tags & nouns from those 20 docs.
3. **Filter by trust** â†’ keep terms seen in content tagged by peers within the askerâ€™s WoT radiusÂ â‰¤â€¯2 hops.
4. **Rocchio reâ€‘weight** â†’Â Î±=1.0 original query, Î²=0.75 feedback vector.
5. **Reâ€‘issue query** â†’ merge results, dedupe, rerank via trustâ€‘weighted scorer.

Latency budget: <30â€¯ms extra; cache expansion terms per userÂ Ã—Â hour.

---

#### 3. Lab â€“ Build & benchmark expansion

* **Dataset**Â Â Use 10â€¯k Pubky posts with groundâ€‘truth relevance judgements.
* **Task**Â Â Compare MRR and nDCG\@10 for baseline vs. expanded queries.
* **Goal**Â Â â‰¥â€¯10â€¯% MRR uplift without >5â€¯% latency hit.

*Code stub*: `expander.py` that wraps your Chapterâ€¯6 router; flag `--expand` toggles logic.

---

#### 4. Quick checklist

* Generate synonym table nightly; store in Redis (< 50 MB).
* Implement pseudo-relevance feedback path behind feature flag.
* Tune Rocchio (Î±, Î²) on validation set.
* Expose /search?expand=1 for A/B testing.
* Log added terms & latency to Prometheus.


  Knock out these tasks and youâ€™ll surface longâ€‘tail content that BM25â€¯+ embeddings alone would missâ€”without overwhelming your latency budget.

### ChapterÂ 8Â â€“Â FullÂ PipelineÂ &Â A/BÂ Test

**Purpose**Â Â Orchestrate ingestÂ â†’ indexÂ â†’ rankÂ â†’ rerankÂ â†’ serve under a 300â€¯ms budgetâ€”and prove the uplift with a statistically solid online experiment.

**Builds on**Â Â Everything you shipped in ChaptersÂ 1â€‘7.

---

#### 1. Integration blueprint

| Stage               | Tech / Responsibility                                                                                                            |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| **Ingest workers**  | Stream posts & tags from Pubky homeservers into [Kafka](https://en.wikipedia.org/wiki/Apache_Kafka) or [NATS](https://nats.io/). |
| **IndexerÂ service** | Update lexical index (BM25), embedding store, trust graph per shard.                                                             |
| **Query router**    | Fanâ€‘out query to shards, combine lexical + semantic results, send to graph reranker.                                             |
| **APIÂ gateway**     | Frontâ€‘door [microservice](https://en.wikipedia.org/wiki/Microservices) that enforces auth, rateâ€‘limits, and canary routing.      |
| **Result cache**    | 95â€‘percentile hits served from Redis/Memcached to shave 30â€¯ms.                                                                   |

Target latency budget:
\*Â Ingest path â‰¤ 1â€¯s from post to searchable.
\*Â 95â€‘pctl query latency â‰¤Â 300â€¯ms, 99â€‘pctl â‰¤Â 500â€¯ms.

---

#### 2. Metrics & observability

* **Query per second (QPS)**, **tail latency** (95â€¯/â€¯99â€¯pctl).
* **[Clickâ€‘through rateÂ (CTR)](https://en.wikipedia.org/wiki/Click-through_rate)** & **nDCG** from implicit feedback.
* **Error budget**Â = 1â€¯â€“â€¯(successful queries Ã· total queries).
* Instrument with [Prometheus](https://prometheus.io/) + Grafana, send alerts to PagerDuty.

---

#### 3. Online experiment playbook

1. **Define hypothesis** â€“ e.g. "Graph rerank increases CTR by â‰¥Â 5â€¯%."
2. **Bucket traffic** â€“ 50â€¯/â€¯50 split via feature flag at API gateway.
3. **Run for exposure** â€“ compute sample size with [pâ€‘value](https://en.wikipedia.org/wiki/P-value) 0.05, powerÂ =Â 0.8.
4. **Analyse** â€“ twoâ€‘tailed zâ€‘test on CTR; monitor guardâ€‘rail metrics (latency, errors).
5. **Rollâ€‘out** â€“ if significant, push to 100â€¯% with a [canary release](https://en.wikipedia.org/wiki/Canary_release) window.

---

#### 4. Handsâ€‘on lab

* Spin up `dockerâ€‘compose up` stack (Kafka/NATS, indexer, router, gateway).
* Replay a oneâ€‘day Pubky feed through ingest; verify document count parity.
* Launch `/search` A/B flags, run synthetic load (locust, k6) to reach 200Â QPS.
* Capture metrics in Prometheus and plot CTR over time in Grafana.

---

#### 5. Quick checklist

* Ship Dockerfiles & \`docker-compose.yml\` for every service.
* Add Prometheus \`/metrics\` endpoint to each service.
* Set latency-budget alerts ( â‰¥ 300 ms p95 ) in Grafana.
* Implement feature-flag toggle and canary ramp-up script.
* Document rollback plan in the runbook.\\

  You now have a productionâ€‘grade, empiricallyâ€‘verified Pubky Search pipeline ready for realâ€‘world traffic.

### GlossaryÂ â€“Â Pubkyâ€‘Centric IR Terms

| Term                                                                                                                           | Plainâ€‘English Definition                                                                                   |
| ------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------- |
| [Inverted Index](https://en.wikipedia.org/wiki/Inverted_index)                                                                 | Dictionary mapping each term to every post where it appears, plus positions. Foundation of lexical search. |
| [TFâ€‘IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)                                                                         | Weight = (term frequency) Ã— log(total docs / docs with term). Firstâ€‘cut relevance score.                   |
| [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) (`k1`,â€¯`b`)                                                                   | Pragmatic scoring that limits termâ€‘frequency saturation (`k1`) and normalises for post length (`b`).       |
| [Sentenceâ€‘BERT](https://www.sbert.net/)                                                                                        | Transformer model that encodes a text snippet to a 384â€‘ or 768â€‘dim vector suitable for similarity search.  |
| [HNSW](https://en.wikipedia.org/wiki/Approximate_nearest_neighbor_search#Hierarchical_navigable_small_world_graphs_%28HNSW%29) | â€œHierarchical Navigable Smallâ€‘Worldâ€ graph; ANN structure for <100â€¯ms nearestâ€‘vector lookâ€‘ups.             |
| [Semantic Social Graph](https://pubky.org)                                                                  | Pubkyâ€™s tripartite graph of usersÂ â†”Â tagsÂ â†”Â posts, enriched with weighted trust edges.                      |
| [PageRank](https://en.wikipedia.org/wiki/PageRank)Â /Â [TrustRank](https://en.wikipedia.org/wiki/TrustRank)                      | Randomâ€‘walk based centrality score. In Pubky we seed walks from the searcherâ€™s trust circle.               |
| [Signed Tag](https://github.com/synonym-dev/pubky)                                                                             | Cryptographically signed label attaching meaning (and trust weight) to a post, user, or file.              |
| [Coâ€‘occurrence Matrix](https://en.wikipedia.org/wiki/Co-occurrence)                                                            | Table counting how often two tags appear together; informs recommendation & query expansion.               |
| [Mainline DHT](https://en.wikipedia.org/wiki/Mainline_DHT)                                                                     | Peerâ€‘toâ€‘peer hash table underlying PKARR; stores keyâ†’value pairs for discovery without servers.            |
| [PKARR](https://github.com/pubky/pkarr)                                                                                  | "DNS for keys" â€“ maps a public key to resource records (homeserver URL, index shards, etc.).               |
| [Nexus Indexer](https://github.com/pubky/pubky-nexus)                                                                    | Pubky microâ€‘service that shards, replicates, and announces search indexes over PKARR.                      |
| [Latency Budget](https://sre.google/sre-book/monitoring-distributed-systems/#latency)                                          | Hard limit (300â€¯ms) for serving a query; each pipeline stage gets a slice.                                 |
| [A/B Test](https://en.wikipedia.org/wiki/A/B_testing)                                                                          | Controlled experiment splitting traffic (e.g., 50/50) to measure CTR or dwellâ€‘time lift.                   |
| [ANN](https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximate_nearest_neighbor_search)                               | Approximate Nearest Neighbour search; trades tiny accuracy loss for huge speed gains on vector search.     |
| [PMI](https://en.wikipedia.org/wiki/Pointwise_mutual_information)                                                              | Pointwise Mutual Information; measures strength of association between two tags or terms.                  |
| [Webâ€‘ofâ€‘Trust](https://en.wikipedia.org/wiki/Web_of_trust)                                                                     | Directed, weighted graph of trust edges between pubkeys, maintained by tagging.                            |
| [Tag Namespace](https://en.wikipedia.org/wiki/Tag_%28metadata%29)                                                              | Logical domain for a tag (e.g., `topic:`, `trust:`), allowing collisions to coexist.                       |
| [Edge Weight](https://en.wikipedia.org/wiki/Graph_%28discrete_mathematics%29#Weighted_graph)                                   | Numeric strength on a graph edge; in Pubky often `trust` or `relevance` scores.                            |
| [Microservice](https://en.wikipedia.org/wiki/Microservices)                                                                    | Small, singleâ€‘purpose network process; how we deploy each stage of the IR pipeline.                        |
